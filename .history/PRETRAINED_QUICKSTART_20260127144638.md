# Pre-trained Model - Quick Start Guide

## âš¡ Start in 5 Minutes (No Training!)

### Step 1: Load Pre-trained Models (30 seconds)

```python
from pretrained_inference import PreTrainedLivenessDetector

# Initialize detector
detector = PreTrainedLivenessDetector(device='cuda')
# or device='cpu' if no GPU
```

**What happens:**
- âœ“ EfficientNet-B3 loaded (pre-trained on ImageNet)
- âœ“ Temporal Transformer initialized
- âœ“ Feature preprocessor ready

---

### Step 2: Predict on Video (2 minutes)

```python
# Predict on any video file
result = detector.predict_video('your_video.mp4')

# Output:
# {
#   'prediction': 'Live',          # or 'Spoof'
#   'score': 0.87,                 # [0, 1]
#   'confidence': 0.92,            # [0, 1]
#   'details': {...}               # detailed stats
# }
```

**Interpretation:**
- Score > 0.7 + Confidence > 0.7 = **Confident LIVE** âœ“
- Score < 0.3 + Confidence > 0.7 = **Confident SPOOF** âœ“
- 0.4 < Score < 0.6 = **UNCERTAIN** âš 

---

### Step 3: Real-time Webcam (2 minutes)

```python
from pretrained_inference import demo_with_webcam

# Start real-time detection
demo_with_webcam()  # Press 'q' to exit
```

**Output:** Live display with predictions per frame

---

## ðŸŽ¯ Three Usage Patterns

### Pattern 1: Quick Single Video

```python
detector = PreTrainedLivenessDetector()
result = detector.predict_video('test.mp4')
print(f"{result['prediction']}: {result['score']:.3f}")
```

### Pattern 2: Batch Processing

```python
videos = ['video1.mp4', 'video2.mp4', 'video3.mp4']

for video_path in videos:
    result = detector.predict_video(video_path, verbose=False)
    print(f"{video_path}: {result['prediction']}")
```

### Pattern 3: Real-time Streaming

```python
import cv2

cap = cv2.VideoCapture(0)  # Webcam

while True:
    ret, frame = cap.read()
    if not ret: break
    
    # Single-frame prediction (fast)
    pred, conf = detector.predict_frame(frame)
    
    # Display
    cv2.putText(frame, f"{pred}: {conf:.2f}", (10, 30), 
               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.imshow('Liveness', frame)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

---

## âš ï¸ Important Note: Accuracy

**Without trained transformer weights:**
- EfficientNet alone: ~65-70% accuracy (baseline CNN)
- With pre-trained transformer: ~85%+ accuracy

**Current status:**
```python
detector = PreTrainedLivenessDetector()
# Transformer using random initialization
# â†’ Results won't be accurate until you train it
```

**To improve accuracy:**

### Option A: Train on Your Data (3-4 hours)
```bash
python train_temporal_transformer.py \
    --data-dir ./your_videos \
    --epochs 50 \
    --batch-size 16 \
    --device cuda

# Then use trained weights:
detector = PreTrainedLivenessDetector(
    transformer_weights='temporal_transformer_best.pt'
)
```

### Option B: Download Pre-trained Weights
(When available from model zoo)
```python
# Future: Download from Hugging Face / PyTorch Hub
model = torch.hub.load('repo', 'temporal_transformer_liveness')
```

### Option C: Use EfficientNet Only (Works now!)
```python
# EfficientNet is already pre-trained on ImageNet
# Just use the baseline CNN without transformer
from models.efficientnet_model import load_efficientnet_model

model = load_efficientnet_model(pretrained=True)
# This will work decently without any training
```

---

## ðŸ“Š Expected Performance

### With Random Transformer (Current)
```
Accuracy:    ~65-70% (CNN baseline)
Confidence:  May be unreliable
Use case:    Demo/testing only
```

### With Trained Transformer (Recommended)
```
Accuracy:    ~85-90%
Confidence:  Reliable
Use case:    Production
```

---

## ðŸš€ Full Example: Start to Production

### Minute 1-5: Test with Pre-trained
```python
from pretrained_inference import PreTrainedLivenessDetector

detector = PreTrainedLivenessDetector()
result = detector.predict_video('test.mp4')
print(result)
# Works immediately!
```

### Hour 1-4: Train on Your Data
```bash
# Record 20-50 videos (10 min)
# Train transformer (3-4 hours)
python train_temporal_transformer.py

# Model saved as: temporal_transformer_best.pt
```

### Hour 5: Use Trained Model
```python
detector = PreTrainedLivenessDetector(
    transformer_weights='temporal_transformer_best.pt'
)

# Now much more accurate!
result = detector.predict_video('new_video.mp4')
```

---

## ðŸ”§ Troubleshooting

### "Transformer weights not found"
```
This is normal! Initialize without weights:
    detector = PreTrainedLivenessDetector()
    
Results won't be perfect, but you can test the pipeline.
```

### "Video file not found"
```
Make sure video path is correct:
    detector.predict_video('./videos/test.mp4')
    # not: detector.predict_video('test.mp4')
```

### "CUDA out of memory"
```
Use CPU instead:
    detector = PreTrainedLivenessDetector(device='cpu')
    # Slower but works on any machine
```

### "No improvement after training"
```
Check your data quality:
    - At least 50 live + 50 spoof videos
    - Good lighting and camera quality
    - Clear face visible in videos
    - Mix of different angles/expressions
```

---

## ðŸ“ˆ Next Steps

### Path 1: Quick Demo (Today)
```bash
python pretrained_inference.py
# See demo_with_webcam() in action
```

### Path 2: Train Custom Model (Tomorrow)
```bash
# Collect 20-50 videos
# Run: python train_temporal_transformer.py
# Wait 3-4 hours
# Use trained model
```

### Path 3: Production Deployment (Next Week)
```bash
# Use trained model with:
#   - Error handling
#   - Logging
#   - Performance monitoring
#   - Confidence thresholds
```

---

## ðŸ’¡ Tips

1. **Start with webcam demo**
   ```bash
   python -c "from pretrained_inference import demo_with_webcam; demo_with_webcam()"
   ```

2. **Test on different videos**
   - High quality videos
   - Blurry videos
   - Compressed videos
   - Different lighting

3. **Monitor confidence metric**
   - High confidence + clear prediction = trust it
   - Low confidence = uncertain, needs more frames

4. **Use single-frame mode for speed**
   ```python
   # Fast (single frame, CNN only)
   pred, conf = detector.predict_frame(frame)
   
   # vs Accurate (12-frame window, full transformer)
   result = detector.predict_video('video.mp4')
   ```

---

## ðŸ“ž Getting Help

| Issue | Solution |
|-------|----------|
| Need better accuracy | Train on your data (see train_temporal_transformer.py) |
| Want to understand model | Read TEMPORAL_TRANSFORMER.md |
| Having bugs | Run diagnostic_temporal_transformer.py |
| Want examples | See quick_integration_example.py |

---

**You're ready to go!** ðŸŽ‰

**Start here:**
```bash
python pretrained_inference.py
```

Then explore the options in the script!
